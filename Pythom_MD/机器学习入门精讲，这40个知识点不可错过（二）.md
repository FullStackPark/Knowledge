# 机器学习入门精讲，这40个知识点不可错过（二）


大家晚上好~

两周前给大家科普了集成学习，很多同学都反馈说还想再学点机器学习的入门知识。

今天晚上就带大家来学习机器学习中另外一个很重要的算法——**决策树**。

在开讲之前，咱们先来看个例子。

![机器学习入门精讲，这40个知识点不可错过（二）](http://p1.pstatp.com/large/pgc-image/1524135569526ae05a266ec)

玩过狼人杀的同学都知道，不管游戏过程多么激烈，该游戏的最终结局只有4种。

为了让同学们更直观地看到这4种结局，班主任画了个非常生动形象（简单粗暴）的图：

![机器学习入门精讲，这40个知识点不可错过（二）](http://p3.pstatp.com/large/pgc-image/152413561412754f06e025b)

这幅图完整表达了狼人杀结局的各种情况，箭头指向一个判断条件在不同情况下的游戏结果，最后通过场上剩余的人来判断是谁获胜。

我们可以看出，在每一个关键节点（比如村人杀死所有的狼人？），依据判断条件，可以将答案划分为Yes or No，最后输出获胜一方。

通过树形结构，根据条件判断输出相应的结局，这种简单的算法，便是决策树的原型。

# **决策树——机器学习中的“倚天宝剑”**

决策树是机器学习中很经典的一种算法。它既是分类算法，也是回归算法，还可以用在随机森林中。

关于随机森林的知识点，可以回顾上期课程机器学习入门精讲，这40个知识点不可错过（一）

咱们学计算机的同学经常敲if 、else if、else其实就已经在用到决策树的思想了。

**决策树是一种简单常用的分类器，通过训练好的决策树可以实现对未知的数据进行高效分类。**

从开头狼人杀的例子中也可以看出，决策树模型具有较好的可读性和描述性，能够帮助我们更高效率地去分析问题。

举个例子，普通人去银行贷款的时候，银行会根据相应条件，来判断贷款人是否具有还贷能力。

贷款用户主要具备三个属性：房产、婚姻、平均月收入。

拥有房产或结过婚或月收入大于4000的贷款用户具备偿还能力。

![机器学习入门精讲，这40个知识点不可错过（二）](http://p3.pstatp.com/large/pgc-image/1524135680295fc05dc4f2b)

例如：用户甲没有房产，没有结婚，月收入5K，通过上图的判断条件可以判断出用户甲具备偿还贷款能力。

这整个判断还贷能力的过程，就用到了决策树的思想。

# **决策树算法——基尼不纯度、熵**

**基尼不纯度**，是指将来自集合中的某种结果随机应用在集合中，某一数据项的预期误差率，可以用来度量任何不均匀分布。  

下图是组合1和组合2的基尼不纯度：

![机器学习入门精讲，这40个知识点不可错过（二）](http://p1.pstatp.com/large/pgc-image/1524135763140e53cf3021a)

从图中我们可以看出，组合1的基尼不纯度大于组合2。

打个比方，基尼不纯度就像男生挑钻石向女生求婚，所有的钻石看着都很闪，但仔细比较价格却差很多，因为每颗钻石的的纯度不一样，纯度越高，价格也就越贵。

由此可总结出这样2种情况：

> A、基尼不纯度越小，纯度越高，集合的有序程度越高，分类的效果越好
>
> B、基尼不纯度为 0 时，表示集合类别一致

**熵度量的是事物的不确定性**，越不确定的事物，它的熵就越大。

熵是信息论中的概念，用来表示集合的无序程度，熵越大表示集合越混乱，反之则表示集合越有序。

# **决策树算法——信息增益**

信息增益越大，说明整个集合从无序到有序的速度越快，本次拆分越有效，则越适合用来分类。

通俗来讲，信息增益代表了在一个条件下，信息复杂度（不确定性）减少的程度。

我们以《非诚勿扰》为例，在男嘉宾出场之前，如果女嘉宾对男嘉宾一无所知，成为他女朋友的不确定性就比较高，如果知道其中的几个特征（如身高、性格等），不确定性就会减少很多。

由上面的例子可见，**一个属性的信息增益越大，表明这个属性使得数据由不确定性变成确定性的能力越强**。

# ****决策树算法的补充要点****<br>

> **关于剪枝**

利用决策树算法构建一个初始的树之后，为了有效的分类，还要对其进行剪枝，剪枝是为了减少过拟合现象。

剪枝思路主要是两种：

**一种是预剪枝**，即在生成决策树的时候就决定是否剪枝。

**另一个是后剪枝**，即先生成决策树，再通过交叉验证来剪枝。

> **关于过拟合**

为了得到一致假设而使假设变得过度复杂称为过拟合。

比如，考试的时候，有的人采用题海战术，把每个题目都背下来。

但是题目稍微变化一下，他就不会做了，因为他没有总结出通用的规则。

过拟合的原因一般有两种：

**A.模型太过复杂而样本量不足**

**B.训练集和测试机特征分布不一致**

好啦，今晚的机器学习中关于决策树的部分就讲到这里了，大家可以好好消化下。点击原文链接，进入微信页面，在留言区写下关于决策树的一些问题和想法，抽10位幸运的童鞋送出「机器学习神秘大礼包」
