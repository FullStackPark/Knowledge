# 机器学习入门精讲，这40个知识点不可错过（一）


大家好~

在刚刚结束的清明小长假中，小伙伴们玩得可还开心？

为了让大家更好地了解和学习AI基础知识，AI大学在清明假期里就人工智能核心领域之一——机器学习系统地总结了其40个入门必备知识点，今晚我们就先来学习第一部分。

**1、集成学习——机器学习中的“屠龙宝刀”**

集成学习方法是使用一系列学习器进行学习，再将学习结果整合，它像是某种优化手段和策略。在机器学习的监督学习算法中，我们想得到的是一个稳定且在各方面表现都很好的模型。

但往往我们只能得到一些弱监督模型（在某些方面表现比较好），集成学习通常就是结合多个简单的弱机器学习算法，去做更准确的决策。

用俗语来讲，就是集众人智慧去做相应的决策，个人的决策可能在某些方面有些不准确，但其他人可以修正他的决策，从而达到结果最优化。

一般来讲集成学习的关键有两点，**一是如何构建不同的分类器，另一个是如何将不同分类器的结果进行融合。**

围绕着这两个核心问题，产生了很多算法，其中最具代表性、最被大家所熟知的就是**Boosting、Bagging和Stacking**。

![机器学习入门精讲，这40个知识点不可错过（一）](http://p1.pstatp.com/large/pgc-image/1523252985783e5e804fe30)

**2、集成学习的算法——Boosting**

Boosting是一种提高任意给定学习算法准确度的方法。它的思想起源于 Valiant提出的 PAC ( Probably Approximately Correct)学习模型。

Boosting是一种框架算法,主要是通过对样本集的操作获得样本子集,然后用弱分类算法在样本子集上训练生成一系列的基分类器。

Adaboost是boosting中较有代表性的算法，**Adaboost是一种迭代算法，通过集合弱分类器，进行训练构成强分类器。**

Adaboost算法的基本流程如下：

> A．用平均分配的方式初始化训练数据
>
> B．选择基本分类器
>
> C．计算分类器的系数
>
> D．更新训练数据的权重分配
>
> E．组合分类器，优化结果

通俗来讲就是由误差率求得分类器系数，由分类器系数得到组合方式。

![机器学习入门精讲，这40个知识点不可错过（一）](http://p3.pstatp.com/large/pgc-image/15232530396625d4ca2aa1f)

**3、集成学习的算法——Bagging**

根据个体学习器的生成方式不同，集成学习的算法分为串行化方法和并行化方法，串行化方法的主要代表就是上面所讲的Boosting——因为个体学习器之间存在强依赖关系，所以只能依次进行。

串行化方法的主要代表就是我们现在要讲的Bagging（bootstrap aggregating简写），因为个体学习器之间的关系依赖没有那么强烈，所以同时进行运算。

打个比方，Boosting更像是我们小时候吃糖葫芦那样，只能先吃掉上面的一颗，才能吃到下面的一颗，而Bagging就像是我们吃面条，可以同时吃很多根，不存在只能先吃哪根，再吃另外一根。

**Bagging的采样方法是自助采样法，用的是有放回的采样。**

一般步骤都是先抽取一定量的样本，再计算想得到的统计量T，重复多次N，得到N个统计量，最后根据统计量，计算统计量的置信区间。

举例说明：比如数据集里有10000个数据，我们随意从中抽取100个，得出统计量T1，然后将这100个数据放回到数据集里，再从中抽取100个数据，得出统计量T2，如此循环反复N次，得出统计量TN，计算出最后的置信区间。

**名词解释：置信区间是指由样本统计量所构造的总体参数的估计区间**

![机器学习入门精讲，这40个知识点不可错过（一）](http://p1.pstatp.com/large/pgc-image/15232530691833a972101b0)

**4、随机森林**

**随机森林（Random Forest）是Bagging的扩展变体。**随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。

简单来说，随机森林相当于Bagging的升级版，**原来的Bagging会在决策树的所有属性中，选择最优的那一个，而随机森林是从相应节点的随机属性中，选择一个最优属性。**

比如森林中有10万棵树，要从中选择高于10米的树，Bagging的做法就是重复统计多次，从而找到高于十米的树的数量区间。

而随机森林的做法是随机将10万棵树分成10份，每份1万棵。对于这10份，每份都有一个输出结果，高出10米或者低于10米。如果高出10米的类别多，则整体是高于10米的，反之亦然。

在概率学中，样本容量越大，结果就越接近，所以随机森林能够在训练效果更高效，计算开销更小的情况下，得出最后结果。

值得注意的是在随机森林中，有两个采样过程是随机的，**第一个是输入数据是随机的**，它是从整体性训练数据中，选取一部分作为决策树的构建，是有放回的选取。（这就保证每棵树都不是全部的样本，不容易出现问题）。

**第二个是每个决策树构建所需特征是从整体特征集中随机选取的**，采样的特征远远小于整体特征。

![机器学习入门精讲，这40个知识点不可错过（一）](http://p3.pstatp.com/large/pgc-image/1523253098276bdbf3b481f)

**5、集成学习的算法——Stacking**

相比较于Bagging和Boosting，Stacking提到的较少，Stacking算法是训练出多个小分类器，把这些小分类器的输出重新组合成为一个新的训练集，训练出来一个更高层次的分类器，以得到最终的结果。

**Stacking算法在实际应用中，通常使用logistic回归作为组合策略。**

Stacking 是一种集成学习技术，通过元分类器或元回归聚合多个分类或回归模型。

基础层次模型（level model）基于完整的训练集进行训练，然后元模型基于基础层次模型的输出进行训练。

![机器学习入门精讲，这40个知识点不可错过（一）](http://p3.pstatp.com/large/pgc-image/1523253126057d8a5de8e11)

好啦，今天的机器学习中关于集成学习的部分就讲到这里了，欢迎大家在**留言区写下关于集成学习的一些问题和想法。**
