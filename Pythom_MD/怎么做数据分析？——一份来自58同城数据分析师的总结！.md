来58两年了，从互联网流量的小白到现在也算是踩了很多坑，很多方法性的东西受制于自己的见识可能无法全面的呈现出来，但是该总结还是要总结的，趁部门有这样的需求，借这个机会理了一遍。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2f0000e246c0fbcbe2)

互联网数据分析的工作其实很成体系。如果一定要将自己手头的事情进行分类的话，可以分成四种：

1.  第一种是在突发的事件产生，比如流量突然下降之后，考虑到及时描述事件做出的描述分析；

2.  第二种是基于产品运营策略上线后，做出的全面的原因的分析；

3.  第三种是在数据管理的过程中，对前后端埋点的校验还有表数据的校验；

4.  第四种是基于一定的理论做出的自上而下的分析；

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p1.pstatp.com/large/6c2d00012414a1801438)

第一种和第二种在描述和寻求原因解释的时候实质上都是一种数据做宽的思想，在做宽的过程中会借助对比、结构来最终描述和解释数据在业务上的变化。

数据的校验分析差异性太大，而且不同的情况处理的有点多，依赖于数据分析师的经验和技术介入能力。

竞品分析则对宏观建构能力有要求，一个好的框架能够帮助自己快速从大量的繁杂且非标的数据中找到切入点，从而迅速找到结论。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p1.pstatp.com/large/66c90004d86e47e76153)

数据做宽既是一种方法，也是一种意识。这么说的原因是基于两个背景，第一个是我们数据存储的形态往往是散落的不同颗粒度长表形态，第二个是我们做出的分析是基于一个颗粒度的指标链条式直观呈现，所以宽表意识是连接数据提取的可能性还有最终产生结论的桥梁。

所以在这个过程中，宽表颗粒度的主键一定要想清楚（活跃企业效果分析），比如活动分析的主键是渠道号、用户流量分析的主键是cookie或者uid、推荐的主键是用户id加上对象的职位id，个人认为分析的过程就是一个做宽表的过程（特别是分析的过程更多的是基于探索的基础上，还有各种四比六分的诉求等等），多个订单中找到首单和尾单指标等等，当我们能够把业务的内容抽象成一张宽表后，我们已经能够将业务目标能够抽象成自己的数据目标了，实际上这种思想运营是最缺少的，运营一直想打通所有的数据。

想清楚这些东西之后才能在每个维度上增加我们想要的衡量指标，比如用户的流量分析，则要把各个环节的行为特征和信息特征都定义出来，这个地方说简单也简单，说难也挺难的，行为特征的定义需要大量业务的知识在里面，才能得出有结论性的内容出来，各个行为特征串起来其实也是一个技术性的问题。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/66c80005d41570c701fd)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2e0000f768a585bbe1)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2e0000f76af22a9b60)

之前做过很多关键事件的盘点分析（BOSS的影响分析、app流量下降分析、M流量异动分析），这些分析过程我们可以理解为是一种基于事件的分析，选中影响的时间轴的先后很重要，然后再选择重点的指标去衡量对比这两个时间先后指标的差异，或者说跟做实验一样，是实验组和对照组选择的过程，选择组+关键指标才是成功得出分析结论的关键，如果很难形成一个比较好的前后时间点比较的内容，也可以拉一个以时间轴为粒度的折线图，这里是最适合代入一些业务的事件东西放进去，如果不清晰的话我们拆分结构再来看折线，拆个几次，有的线会稳定，有的会存在较大的波动，波动的维度就是需要重点考虑的。

指标影响一般都是围绕我们关键想分析的内容，BOSS影响分析更多围绕发布流量和活跃指标来讲的，app流量下降分析指标是既定的，所以找了很多结构性的内容来辅助说明，比如新老用户、app的强制注册策略，但是多是基于业务理解的。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2d0001257f27778d92)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2d000127820f29a7c8)

在准确的数据不充分或者横向比较的时候，特别是做的（竞品分析，繁星导入对平台影响分析、前后对比)会用到大量的结构分析比较，结构分析用柱状图比较多，优势结构和劣势结构一目了然，然后这种分析方式也避免了一直对数的毛病，缺点就是不精确 。

之前验证的关键影响因子的内容都需要在探索性的分析中尽量有体现，特别是在找原因的过程中，之前验证的有影响新老用户占比、是否为付费渠道占比、是否为经过营销补贴的用户占比等等都需要加入到宽表中。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2f0000e70b6d82aa25)

新产品验证或者运营策略用漏斗会比较多，这一套东西跟增长黑客的理论会很像，这里的数据如何定义流程节点的行为很关键，流程节点和底层MVP埋点现实情况需求的数据结合起来，总之就是在镣铐下舞蹈，在各种限制性条件来解决实际问题。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/66c90004dd062a1f9a89)

密度分析的方法不常用，我用的时候主要用在标签很多密集的时候，比如打电话时长分布，用户为主键自然是一个分布形态，本质上如果标签够少就是一个柱状图的分布，这里用法跟卡方类似，如果比较其他的分类变量比如性别或者渠道等等，这里的分布偏向则一目了然，或者说，这也是对于平均数的一种拓展。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p1.pstatp.com/large/6c2e0000fbb46f8fffba)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p1.pstatp.com/large/66c80005d8ddec3c92bf)

校验数据最大的难点在于需要自己找到锚定的指标来衡量这个数据的准确性（其中难度最大还是在终态表的校验，跟业务的代码有关，流水表倒可以通过个案测试来实现），然后发现问题之后尝试了解这个数据的原理，看有没有修正的可能，比如-1来源的简历到底是什么（通过结构占比、注册来源、简历完整度来衡量），简历表中的刷新时间到底有没有包括登陆后的自动刷新（找到登陆时间来看结构占比），职位付费状态和企业付费状态是一回事吗（对比一致的情况，对比不一致的情况，通过交叉来查看，如果继续出现问题，比如一个企业既有免费职位又有付费职位，则可以通过注册时间进一步排查）等等。

* 埋点

* 传输

* 统计口径

比如-1简历来源问题，前面两个过程都是帮我们判断这个到底是不是系统的bug数据问题。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c30000083e1d63aa564)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p1.pstatp.com/large/6c2d00012ae8fd122983)

竞品分析比较依赖于自上而下的模型，因为不同数据的对比是非标且海量的，纯粹探索成本太高且效果不好。

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p3.pstatp.com/large/6c2e0000fd931f7725a6)

![怎么做数据分析？——一份来自58同城数据分析师的总结！](http://p9.pstatp.com/large/6c2c0001f015f8749a49)
